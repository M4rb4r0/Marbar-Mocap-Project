# Proyecto Mocap - Sistema de Captura de Movimiento

Sistema de captura de movimiento (Motion Capture) con detecciÃ³n de:
- **Torso y brazos** (pose corporal)
- **Manos** (dedos y gestos)
- **Expresiones faciales** (ojos, boca, cejas)

Actualmente usando 1 sola webcam

## Librerias
Importante: Usar python 3.10 para compatibilidad con PyTorch y MediaPipe
- **PyTorch** + CUDA para procesamiento
- **MediaPipe** para detecciÃ³n de pose, manos y cara
- **OpenCV** para captura y procesamiento de video
- **NumPy/SciPy** para triangulaciÃ³n 3D y transformaciones
- **Matplotlib** para visualizaciÃ³n 3D

## Estructura del Proyecto

```
Proyecto-Mocap/
â”œâ”€â”€ pose/                     # DetecciÃ³n MediaPipe
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ body_detector.py      # DetecciÃ³n de torso/brazos (33 landmarks)
â”‚   â”œâ”€â”€ hand_detector.py      # DetecciÃ³n de manos (21 landmarks x 2)
â”‚   â”œâ”€â”€ face_detector.py      # DetecciÃ³n facial (468 landmarks)
â”‚   â””â”€â”€ unified_detector.py   # Integrador de todos los detectores
â”‚
â”œâ”€â”€ scripts/                  # Utilidades
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ camera_utils.py       # Captura de cÃ¡mara
â”‚   â””â”€â”€ data_export.py        # ExportaciÃ³n JSON/NumPy
â”‚
â”œâ”€â”€ data/                     # Datos capturados
â”‚   â””â”€â”€ recordings/           # Sesiones grabadas
â”‚
â”œâ”€â”€ logs/                     # Logs del sistema
â”œâ”€â”€ config.yaml               # ConfiguraciÃ³n principal
â”œâ”€â”€ requirements.txt          # Dependencias
â”œâ”€â”€ main.py                   # Script principal
â”œâ”€â”€ test_quick.py             # Test rÃ¡pido
â””â”€â”€ README.md                 # DocumentaciÃ³n completa
```

## âš™ï¸ InstalaciÃ³n

### 1. Crear y activar venv
```powershell
  py -3.10 -m venv venv
  .\mocap\Scripts\Activate.ps1
```

### 2. Instalar dependencias
```powershell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install opencv-python mediapipe numpy scipy onnxruntime-gpu matplotlib PyYAML
```

## Uso de CÃ¡mara Ãºnica

### OpciÃ³n 1: Captura en tiempo real
```powershell
python main.py --mode live
```
Abre la cÃ¡mara y muestra detecciones en tiempo real (pose + manos + cara).

### OpciÃ³n 2: Grabar sesiÃ³n
```powershell
python main.py --mode record --output data/recordings/Mocap_Record_001
```
Graba video con detecciones superpuestas y guarda landmarks en JSON.

### OpciÃ³n 3: Procesar video existente
```powershell
python main.py --mode process --input video.mp4
```
Procesa un video y extrae landmarks de pose, manos y cara.

### OpciÃ³n 4: Visualizar datos guardados
```powershell
python main.py --mode visualize --input data/recordings/session_001
```

## Workflow Actual (Fase 1)

1. **ConfiguraciÃ³n inicial**: Editar `config.yaml` con ID de cÃ¡mara (por defecto: 0)
2. **Prueba en vivo**: Ejecutar `python main.py --mode live` para ver detecciones en tiempo real
3. **Grabar sesiones**: Usar `--mode record` para guardar video + landmarks
4. **Analiza datos**: Los landmarks 2D se guardan en JSON para anÃ¡lisis o integraciÃ³n

## PrÃ³xima Fase: MulticÃ¡mara

Una vez validado el sistema con 1 cÃ¡mara:
1. Agregar segunda cÃ¡mara a `config.yaml`
2. Calibrar cÃ¡maras (relaciÃ³n espacial entre ellas)
3. Triangular puntos 2D â†’ 3D
4. Exportar a formato 3D (BVH/FBX) para Unity/Blender

## ConfiguraciÃ³n de CÃ¡mara

Editar `config.yaml` para ajustar:
- `camera_ids`: Ãndice USB (ej: [0]) o URL RTSP
- `resolution`: ResoluciÃ³n de captura (por defecto: [1280, 720])
- `fps`: Frames por segundo (recomendado: 30)

Para encontrar cÃ¡mara:
```powershell
python -c "import cv2; print([i for i in range(4) if cv2.VideoCapture(i).isOpened()])"
```

## Outputs

Los datos capturados se guardan en:
- `data/recordings/<session_name>/`
  - `camera_<id>/frames/`: Frames de cada cÃ¡mara
  - `camera_<id>/detections_2d.json`: Landmarks 2D por frame
  - `reconstruction_3d.json`: Coordenadas 3D trianguladas
  - `metadata.json`: Info de la sesiÃ³n

## ğŸ“Š Formato de Datos 2D

Ejemplo de estructura JSON:
```json
{
  "frame_0": {
    "timestamp": 0.0,
    "body": [[x, y, visibility], ...],  // 33 landmarks de pose (coords normalizadas 0-1)
    "hands": {
      "left": [[x, y, z], ...],   // 21 landmarks por mano (z es profundidad relativa)
      "right": [[x, y, z], ...]
    },
    "face": [[x, y, z], ...]  // 468 landmarks faciales
  }
}
```

Nota: Con una cÃ¡mara obtenemos coordenadas 2D (x, y) normalizadas. MediaPipe tambiÃ©n proporciona profundidad relativa (z).

## Troubleshooting

- **CÃ¡maras no detectadas**: Verificar los IDs con `python -c "import cv2; print([i for i in range(4) if cv2.VideoCapture(i).isOpened()])"`
- **Baja FPS**: Reducir la resoluciÃ³n o desactiva detecciones no necesarias en `config.yaml`

## Roadmap

### Fase 1: CÃ¡mara Ãºnica (Actual)
- [x] ConfiguraciÃ³n base del proyecto
- [x] DetecciÃ³n de pose, manos y cara con MediaPipe
- [x] Captura en tiempo real con overlay
- [x] GrabaciÃ³n de sesiones con timestamps
- [x] ExportaciÃ³n de landmarks 2D a JSON

### Fase 2: MulticÃ¡mara
- [ ] CalibraciÃ³n de cÃ¡maras mÃºltiples
- [ ] SincronizaciÃ³n de frames
- [ ] TriangulaciÃ³n 2D â†’ 3D
- [ ] VisualizaciÃ³n 3D en tiempo real

### Fase 3: IntegraciÃ³n
- [ ] ExportaciÃ³n a BVH/FBX
- [ ] Plugin para Unity (streaming en tiempo real)
- [ ] Filtro de Kalman para suavizado
- [ ] GUI de control

## ğŸ“ Notas

- Calibra las cÃ¡maras cada vez que se cambie su posiciÃ³n
- Para mejores resultados, usar iluminaciÃ³n uniforme y fondo de 1 solo color
